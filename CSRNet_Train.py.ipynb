{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78262826-cf68-4fd9-a307-33fd937aa8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Training new model...\n",
      "Training for 5 epochs...\n",
      "Epoch 1/5, Loss: 0.061510, LR: 1.00e-05\n",
      "Best model saved with loss: 0.061510\n",
      "Epoch 2/5, Loss: 0.053974, LR: 1.00e-05\n",
      "Best model saved with loss: 0.053974\n",
      "Epoch 3/5, Loss: 0.042215, LR: 1.00e-05\n",
      "Best model saved with loss: 0.042215\n",
      "Epoch 4/5, Loss: 0.040385, LR: 1.00e-05\n",
      "Best model saved with loss: 0.040385\n",
      "Epoch 5/5, Loss: 0.037369, LR: 5.00e-06\n",
      "Best model saved with loss: 0.037369\n",
      "Final model saved to csrnet_partA_final.pth\n",
      "Evaluating model...\n",
      "MAE: 118.39, RMSE: 133.38\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 224 (Limit: 30)\n",
      "IMG_1.jpg            | Count:  224 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 240 (Limit: 30)\n",
      "IMG_10.jpg           | Count:  240 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 302 (Limit: 30)\n",
      "IMG_100.jpg          | Count:  302 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 205 (Limit: 30)\n",
      "IMG_101.jpg          | Count:  205 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 271 (Limit: 30)\n",
      "IMG_102.jpg          | Count:  271 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 201 (Limit: 30)\n",
      "IMG_103.jpg          | Count:  201 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 483 (Limit: 30)\n",
      "IMG_104.jpg          | Count:  483 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 224 (Limit: 30)\n",
      "IMG_105.jpg          | Count:  224 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 473 (Limit: 30)\n",
      "IMG_106.jpg          | Count:  473 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 174 (Limit: 30)\n",
      "IMG_107.jpg          | Count:  174 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 227 (Limit: 30)\n",
      "IMG_108.jpg          | Count:  227 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 225 (Limit: 30)\n",
      "IMG_109.jpg          | Count:  225 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 321 (Limit: 30)\n",
      "IMG_11.jpg           | Count:  321 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 257 (Limit: 30)\n",
      "IMG_110.jpg          | Count:  257 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 225 (Limit: 30)\n",
      "IMG_111.jpg          | Count:  225 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 286 (Limit: 30)\n",
      "IMG_112.jpg          | Count:  286 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 233 (Limit: 30)\n",
      "IMG_113.jpg          | Count:  233 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 174 (Limit: 30)\n",
      "IMG_114.jpg          | Count:  174 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 373 (Limit: 30)\n",
      "IMG_115.jpg          | Count:  373 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 323 (Limit: 30)\n",
      "IMG_116.jpg          | Count:  323 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 522 (Limit: 30)\n",
      "IMG_117.jpg          | Count:  522 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 216 (Limit: 30)\n",
      "IMG_118.jpg          | Count:  216 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 219 (Limit: 30)\n",
      "IMG_119.jpg          | Count:  219 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 421 (Limit: 30)\n",
      "IMG_12.jpg           | Count:  421 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 176 (Limit: 30)\n",
      "IMG_120.jpg          | Count:  176 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 447 (Limit: 30)\n",
      "IMG_121.jpg          | Count:  447 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 224 (Limit: 30)\n",
      "IMG_122.jpg          | Count:  224 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 196 (Limit: 30)\n",
      "IMG_123.jpg          | Count:  196 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 199 (Limit: 30)\n",
      "IMG_124.jpg          | Count:  199 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 271 (Limit: 30)\n",
      "IMG_125.jpg          | Count:  271 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 211 (Limit: 30)\n",
      "IMG_126.jpg          | Count:  211 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 489 (Limit: 30)\n",
      "IMG_127.jpg          | Count:  489 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 253 (Limit: 30)\n",
      "IMG_128.jpg          | Count:  253 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 163 (Limit: 30)\n",
      "IMG_129.jpg          | Count:  163 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 221 (Limit: 30)\n",
      "IMG_13.jpg           | Count:  221 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 308 (Limit: 30)\n",
      "IMG_130.jpg          | Count:  308 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 240 (Limit: 30)\n",
      "IMG_131.jpg          | Count:  240 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 255 (Limit: 30)\n",
      "IMG_132.jpg          | Count:  255 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 208 (Limit: 30)\n",
      "IMG_133.jpg          | Count:  208 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 151 (Limit: 30)\n",
      "IMG_134.jpg          | Count:  151 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 154 (Limit: 30)\n",
      "IMG_135.jpg          | Count:  154 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 163 (Limit: 30)\n",
      "IMG_136.jpg          | Count:  163 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 349 (Limit: 30)\n",
      "IMG_137.jpg          | Count:  349 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 220 (Limit: 30)\n",
      "IMG_138.jpg          | Count:  220 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 189 (Limit: 30)\n",
      "IMG_139.jpg          | Count:  189 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 259 (Limit: 30)\n",
      "IMG_14.jpg           | Count:  259 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 159 (Limit: 30)\n",
      "IMG_140.jpg          | Count:  159 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 238 (Limit: 30)\n",
      "IMG_141.jpg          | Count:  238 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 200 (Limit: 30)\n",
      "IMG_142.jpg          | Count:  200 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 297 (Limit: 30)\n",
      "IMG_143.jpg          | Count:  297 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 271 (Limit: 30)\n",
      "IMG_144.jpg          | Count:  271 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 290 (Limit: 30)\n",
      "IMG_145.jpg          | Count:  290 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 239 (Limit: 30)\n",
      "IMG_146.jpg          | Count:  239 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 393 (Limit: 30)\n",
      "IMG_147.jpg          | Count:  393 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 165 (Limit: 30)\n",
      "IMG_148.jpg          | Count:  165 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 241 (Limit: 30)\n",
      "IMG_149.jpg          | Count:  241 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 251 (Limit: 30)\n",
      "IMG_15.jpg           | Count:  251 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 483 (Limit: 30)\n",
      "IMG_150.jpg          | Count:  483 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 242 (Limit: 30)\n",
      "IMG_151.jpg          | Count:  242 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 244 (Limit: 30)\n",
      "IMG_152.jpg          | Count:  244 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 214 (Limit: 30)\n",
      "IMG_153.jpg          | Count:  214 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 301 (Limit: 30)\n",
      "IMG_154.jpg          | Count:  301 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 199 (Limit: 30)\n",
      "IMG_155.jpg          | Count:  199 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 183 (Limit: 30)\n",
      "IMG_156.jpg          | Count:  183 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 233 (Limit: 30)\n",
      "IMG_157.jpg          | Count:  233 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 308 (Limit: 30)\n",
      "IMG_158.jpg          | Count:  308 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 252 (Limit: 30)\n",
      "IMG_159.jpg          | Count:  252 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 320 (Limit: 30)\n",
      "IMG_16.jpg           | Count:  320 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 159 (Limit: 30)\n",
      "IMG_160.jpg          | Count:  159 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 170 (Limit: 30)\n",
      "IMG_161.jpg          | Count:  170 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 217 (Limit: 30)\n",
      "IMG_162.jpg          | Count:  217 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 227 (Limit: 30)\n",
      "IMG_163.jpg          | Count:  227 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 301 (Limit: 30)\n",
      "IMG_164.jpg          | Count:  301 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 201 (Limit: 30)\n",
      "IMG_165.jpg          | Count:  201 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 201 (Limit: 30)\n",
      "IMG_166.jpg          | Count:  201 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 189 (Limit: 30)\n",
      "IMG_167.jpg          | Count:  189 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 215 (Limit: 30)\n",
      "IMG_168.jpg          | Count:  215 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 255 (Limit: 30)\n",
      "IMG_169.jpg          | Count:  255 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 417 (Limit: 30)\n",
      "IMG_17.jpg           | Count:  417 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 284 (Limit: 30)\n",
      "IMG_170.jpg          | Count:  284 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 212 (Limit: 30)\n",
      "IMG_171.jpg          | Count:  212 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 379 (Limit: 30)\n",
      "IMG_172.jpg          | Count:  379 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 283 (Limit: 30)\n",
      "IMG_173.jpg          | Count:  283 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 196 (Limit: 30)\n",
      "IMG_174.jpg          | Count:  196 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 219 (Limit: 30)\n",
      "IMG_175.jpg          | Count:  219 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 384 (Limit: 30)\n",
      "IMG_176.jpg          | Count:  384 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 186 (Limit: 30)\n",
      "IMG_177.jpg          | Count:  186 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 307 (Limit: 30)\n",
      "IMG_178.jpg          | Count:  307 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 173 (Limit: 30)\n",
      "IMG_179.jpg          | Count:  173 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 238 (Limit: 30)\n",
      "IMG_18.jpg           | Count:  238 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 354 (Limit: 30)\n",
      "IMG_180.jpg          | Count:  354 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 212 (Limit: 30)\n",
      "IMG_181.jpg          | Count:  212 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 173 (Limit: 30)\n",
      "IMG_182.jpg          | Count:  173 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 278 (Limit: 30)\n",
      "IMG_19.jpg           | Count:  278 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 401 (Limit: 30)\n",
      "IMG_2.jpg            | Count:  401 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 320 (Limit: 30)\n",
      "IMG_20.jpg           | Count:  320 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 216 (Limit: 30)\n",
      "IMG_21.jpg           | Count:  216 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 240 (Limit: 30)\n",
      "IMG_22.jpg           | Count:  240 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 206 (Limit: 30)\n",
      "IMG_23.jpg           | Count:  206 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 277 (Limit: 30)\n",
      "IMG_24.jpg           | Count:  277 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 169 (Limit: 30)\n",
      "IMG_25.jpg           | Count:  169 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 233 (Limit: 30)\n",
      "IMG_26.jpg           | Count:  233 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 374 (Limit: 30)\n",
      "IMG_27.jpg           | Count:  374 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 192 (Limit: 30)\n",
      "IMG_28.jpg           | Count:  192 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 269 (Limit: 30)\n",
      "IMG_29.jpg           | Count:  269 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 226 (Limit: 30)\n",
      "IMG_3.jpg            | Count:  226 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 264 (Limit: 30)\n",
      "IMG_30.jpg           | Count:  264 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 240 (Limit: 30)\n",
      "IMG_31.jpg           | Count:  240 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 249 (Limit: 30)\n",
      "IMG_32.jpg           | Count:  249 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 176 (Limit: 30)\n",
      "IMG_33.jpg           | Count:  176 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 180 (Limit: 30)\n",
      "IMG_34.jpg           | Count:  180 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 183 (Limit: 30)\n",
      "IMG_35.jpg           | Count:  183 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 463 (Limit: 30)\n",
      "IMG_36.jpg           | Count:  463 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 177 (Limit: 30)\n",
      "IMG_37.jpg           | Count:  177 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 231 (Limit: 30)\n",
      "IMG_38.jpg           | Count:  231 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 177 (Limit: 30)\n",
      "IMG_39.jpg           | Count:  177 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 202 (Limit: 30)\n",
      "IMG_4.jpg            | Count:  202 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 374 (Limit: 30)\n",
      "IMG_40.jpg           | Count:  374 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 165 (Limit: 30)\n",
      "IMG_41.jpg           | Count:  165 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 277 (Limit: 30)\n",
      "IMG_42.jpg           | Count:  277 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 174 (Limit: 30)\n",
      "IMG_43.jpg           | Count:  174 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 208 (Limit: 30)\n",
      "IMG_44.jpg           | Count:  208 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 195 (Limit: 30)\n",
      "IMG_45.jpg           | Count:  195 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 189 (Limit: 30)\n",
      "IMG_46.jpg           | Count:  189 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 321 (Limit: 30)\n",
      "IMG_47.jpg           | Count:  321 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 262 (Limit: 30)\n",
      "IMG_48.jpg           | Count:  262 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 241 (Limit: 30)\n",
      "IMG_49.jpg           | Count:  241 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 370 (Limit: 30)\n",
      "IMG_5.jpg            | Count:  370 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 331 (Limit: 30)\n",
      "IMG_50.jpg           | Count:  331 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 269 (Limit: 30)\n",
      "IMG_51.jpg           | Count:  269 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 173 (Limit: 30)\n",
      "IMG_52.jpg           | Count:  173 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 207 (Limit: 30)\n",
      "IMG_53.jpg           | Count:  207 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 293 (Limit: 30)\n",
      "IMG_54.jpg           | Count:  293 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 157 (Limit: 30)\n",
      "IMG_55.jpg           | Count:  157 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 203 (Limit: 30)\n",
      "IMG_56.jpg           | Count:  203 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 242 (Limit: 30)\n",
      "IMG_57.jpg           | Count:  242 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 189 (Limit: 30)\n",
      "IMG_58.jpg           | Count:  189 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 324 (Limit: 30)\n",
      "IMG_59.jpg           | Count:  324 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 305 (Limit: 30)\n",
      "IMG_6.jpg            | Count:  305 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 540 (Limit: 30)\n",
      "IMG_60.jpg           | Count:  540 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 213 (Limit: 30)\n",
      "IMG_61.jpg           | Count:  213 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 254 (Limit: 30)\n",
      "IMG_62.jpg           | Count:  254 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 283 (Limit: 30)\n",
      "IMG_63.jpg           | Count:  283 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 215 (Limit: 30)\n",
      "IMG_64.jpg           | Count:  215 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 200 (Limit: 30)\n",
      "IMG_65.jpg           | Count:  200 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 295 (Limit: 30)\n",
      "IMG_66.jpg           | Count:  295 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 201 (Limit: 30)\n",
      "IMG_67.jpg           | Count:  201 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 184 (Limit: 30)\n",
      "IMG_68.jpg           | Count:  184 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 202 (Limit: 30)\n",
      "IMG_69.jpg           | Count:  202 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 465 (Limit: 30)\n",
      "IMG_7.jpg            | Count:  465 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 259 (Limit: 30)\n",
      "IMG_70.jpg           | Count:  259 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 235 (Limit: 30)\n",
      "IMG_71.jpg           | Count:  235 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 320 (Limit: 30)\n",
      "IMG_72.jpg           | Count:  320 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 173 (Limit: 30)\n",
      "IMG_73.jpg           | Count:  173 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 466 (Limit: 30)\n",
      "IMG_74.jpg           | Count:  466 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 233 (Limit: 30)\n",
      "IMG_75.jpg           | Count:  233 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 405 (Limit: 30)\n",
      "IMG_76.jpg           | Count:  405 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 176 (Limit: 30)\n",
      "IMG_77.jpg           | Count:  176 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 215 (Limit: 30)\n",
      "IMG_78.jpg           | Count:  215 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 192 (Limit: 30)\n",
      "IMG_79.jpg           | Count:  192 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 362 (Limit: 30)\n",
      "IMG_8.jpg            | Count:  362 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 170 (Limit: 30)\n",
      "IMG_80.jpg           | Count:  170 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 361 (Limit: 30)\n",
      "IMG_81.jpg           | Count:  361 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 179 (Limit: 30)\n",
      "IMG_82.jpg           | Count:  179 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 213 (Limit: 30)\n",
      "IMG_83.jpg           | Count:  213 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 186 (Limit: 30)\n",
      "IMG_84.jpg           | Count:  186 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 177 (Limit: 30)\n",
      "IMG_85.jpg           | Count:  177 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 243 (Limit: 30)\n",
      "IMG_86.jpg           | Count:  243 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 193 (Limit: 30)\n",
      "IMG_87.jpg           | Count:  193 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 158 (Limit: 30)\n",
      "IMG_88.jpg           | Count:  158 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 272 (Limit: 30)\n",
      "IMG_89.jpg           | Count:  272 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 199 (Limit: 30)\n",
      "IMG_9.jpg            | Count:  199 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 501 (Limit: 30)\n",
      "IMG_90.jpg           | Count:  501 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 163 (Limit: 30)\n",
      "IMG_91.jpg           | Count:  163 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 424 (Limit: 30)\n",
      "IMG_92.jpg           | Count:  424 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 212 (Limit: 30)\n",
      "IMG_93.jpg           | Count:  212 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 156 (Limit: 30)\n",
      "IMG_94.jpg           | Count:  156 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 192 (Limit: 30)\n",
      "IMG_95.jpg           | Count:  192 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 261 (Limit: 30)\n",
      "IMG_96.jpg           | Count:  261 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 368 (Limit: 30)\n",
      "IMG_97.jpg           | Count:  368 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 175 (Limit: 30)\n",
      "IMG_98.jpg           | Count:  175 | ðŸš¨ ALERT\n",
      "ðŸš¨ ALERT: Overcrowded! Count: 163 (Limit: 30)\n",
      "IMG_99.jpg           | Count:  163 | ðŸš¨ ALERT\n",
      "Summary: 182/182 images triggered alerts\n",
      "Alert percentage: 100.0%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from scipy.io import loadmat\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# -------------------------------\n",
    "# Config\n",
    "# -------------------------------\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "TRAIN_IMG_DIR = r\"D:\\Task For Infosys Internship\\archive\\ShanghaiTech\\part_A\\train_data\\images\"\n",
    "TRAIN_GT_DIR = r\"D:\\Task For Infosys Internship\\archive\\ShanghaiTech\\part_A\\train_data\\ground-truth\"\n",
    "TEST_IMG_DIR  = r\"D:\\Task For Infosys Internship\\archive\\ShanghaiTech\\part_A\\test_data\\images\"\n",
    "TEST_GT_DIR   = r\"D:\\Task For Infosys Internship\\archive\\ShanghaiTech\\part_A\\test_data\\ground-truth\"\n",
    "\n",
    "IMG_HEIGHT = 256\n",
    "IMG_WIDTH = 256\n",
    "DOWNSAMPLE_FACTOR = 8\n",
    "OUTPUT_SIZE = IMG_HEIGHT // DOWNSAMPLE_FACTOR\n",
    "\n",
    "# -------------------------------\n",
    "# Gaussian Density Map Generator\n",
    "# -------------------------------\n",
    "def gaussian_filter_density(gt):\n",
    "    density = np.zeros(gt.shape, dtype=np.float32)\n",
    "    pts = np.array(list(zip(np.nonzero(gt)[1], np.nonzero(gt)[0])))\n",
    "    if len(pts) == 0:\n",
    "        return density\n",
    "    sigma = 15\n",
    "    for i in range(len(pts)):\n",
    "        pt2d = np.zeros(gt.shape, dtype=np.float32)\n",
    "        y, x = pts[i][1], pts[i][0]\n",
    "        if y < gt.shape[0] and x < gt.shape[1]:\n",
    "            pt2d[y, x] = 1.\n",
    "        density += cv2.GaussianBlur(pt2d, (0,0), sigma, borderType=cv2.BORDER_CONSTANT)\n",
    "    return density\n",
    "\n",
    "# -------------------------------\n",
    "# Dataset\n",
    "# -------------------------------\n",
    "class CrowdDataset(Dataset):\n",
    "    def __init__(self, img_dir, gt_dir, transform=None, img_size=(IMG_HEIGHT, IMG_WIDTH)):\n",
    "        self.img_paths = glob.glob(os.path.join(img_dir, \"*.jpg\"))\n",
    "        self.gt_dir = gt_dir\n",
    "        self.transform = transform\n",
    "        self.img_size = img_size\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        filename_mat = os.path.basename(img_path).replace(\".jpg\", \".mat\")\n",
    "        filename_h5  = os.path.basename(img_path).replace(\".jpg\", \".h5\")\n",
    "        mat_path     = os.path.join(self.gt_dir, \"GT_\" + filename_mat)\n",
    "        h5_path      = os.path.join(self.gt_dir, \"GT_\" + filename_h5)\n",
    "        \n",
    "        # Try both .mat (original annotation) and .h5 (density map) file naming\n",
    "        if os.path.exists(h5_path):\n",
    "            with h5py.File(h5_path, 'r') as hf:\n",
    "                # Typical structure: 'density'\n",
    "                density = np.asarray(hf['density'])\n",
    "                img = img.resize(self.img_size)\n",
    "                if self.transform:\n",
    "                    img = self.transform(img)\n",
    "                density = cv2.resize(density, self.img_size, interpolation=cv2.INTER_LINEAR)\n",
    "                density = torch.from_numpy(density).unsqueeze(0).float()\n",
    "                return img, density\n",
    "        elif os.path.exists(mat_path):\n",
    "            try:\n",
    "                mat = loadmat(mat_path)\n",
    "                points = mat['image_info'][0,0]['location'][0,0]\n",
    "            except NotImplementedError:\n",
    "                with h5py.File(mat_path, 'r') as f:\n",
    "                    points = np.array(f['image_info'][0,0][0,0][0])\n",
    "            if points.size == 0:\n",
    "                coords = np.empty((0, 2), dtype=np.int32)\n",
    "            else:\n",
    "                coords = np.array(points)\n",
    "            img = img.resize(self.img_size)\n",
    "            h, w = img.size[1], img.size[0]\n",
    "            k = np.zeros((h, w))\n",
    "            for i in range(coords.shape[0]):\n",
    "                x = min(int(coords[i][0]), w-1)\n",
    "                y = min(int(coords[i][1]), h-1)\n",
    "                if y < h and x < w:\n",
    "                    k[y, x] = 1\n",
    "            density = gaussian_filter_density(k)\n",
    "            scale_factor = (self.img_size[0] * self.img_size[1]) / (h * w)\n",
    "            density = cv2.resize(density, self.img_size, interpolation=cv2.INTER_LINEAR)\n",
    "            density = density * scale_factor\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            density = torch.from_numpy(density).unsqueeze(0).float()\n",
    "            return img, density\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Ground truth file not found: {mat_path} or {h5_path}\")\n",
    "\n",
    "# -------------------------------\n",
    "# CSRNet Model\n",
    "# -------------------------------\n",
    "class CSRNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CSRNet, self).__init__()\n",
    "        vgg = models.vgg16_bn(weights=models.VGG16_BN_Weights.DEFAULT)\n",
    "        self.frontend = nn.Sequential(*list(vgg.features.children())[:33]) \n",
    "        self.backend = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, 3, padding=2, dilation=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, 3, padding=2, dilation=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, 3, padding=2, dilation=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 256, 3, padding=2, dilation=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 128, 3, padding=2, dilation=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 64, 3, padding=2, dilation=2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.output_layer = nn.Conv2d(64, 1, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        x = self.frontend(x)\n",
    "        x = self.backend(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "# -------------------------------\n",
    "# Training\n",
    "# -------------------------------\n",
    "def train_model(num_epochs=5, batch_size=4, learning_rate=1e-5, save_every=5):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "    ])\n",
    "    train_dataset = CrowdDataset(TRAIN_IMG_DIR, TRAIN_GT_DIR, transform=transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    model = CSRNet().to(DEVICE)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "    print(f\"Training for {num_epochs} epochs...\")\n",
    "    best_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        batch_count = 0\n",
    "        for imgs, densities in train_loader:\n",
    "            imgs, densities = imgs.to(DEVICE), densities.to(DEVICE)\n",
    "            target_densities_downsampled = F.interpolate(\n",
    "                densities, \n",
    "                size=(OUTPUT_SIZE, OUTPUT_SIZE), \n",
    "                mode='bilinear', \n",
    "                align_corners=False\n",
    "            )\n",
    "            target_densities_downsampled = target_densities_downsampled * (DOWNSAMPLE_FACTOR ** 2)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, target_densities_downsampled) \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            batch_count += 1\n",
    "        avg_loss = epoch_loss / batch_count\n",
    "        scheduler.step()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.6f}, LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(model.state_dict(), \"csrnet_partA_best.pth\")\n",
    "            print(f\"Best model saved with loss: {best_loss:.6f}\")\n",
    "        if (epoch + 1) % save_every == 0:\n",
    "            torch.save(model.state_dict(), f\"csrnet_partA_epoch_{epoch+1}.pth\")\n",
    "    torch.save(model.state_dict(), \"csrnet_partA_final.pth\")\n",
    "    print(\"Final model saved to csrnet_partA_final.pth\")\n",
    "    return model\n",
    "\n",
    "# -------------------------------\n",
    "# Evaluation\n",
    "# -------------------------------\n",
    "def evaluate_model(model):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "    ])\n",
    "    val_dataset = CrowdDataset(TEST_IMG_DIR, TEST_GT_DIR, transform=transform)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "    print(\"Evaluating model...\")\n",
    "    mae, rmse, n = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, densities in val_loader:\n",
    "            imgs, densities = imgs.to(DEVICE), densities.to(DEVICE)\n",
    "            outputs = model(imgs)\n",
    "            predicted_count = max(outputs.sum().item(), 0)\n",
    "            actual_count = densities.sum().item()\n",
    "            mae += abs(predicted_count - actual_count)\n",
    "            rmse += (predicted_count - actual_count) ** 2\n",
    "            n += 1\n",
    "    mae /= n\n",
    "    rmse = np.sqrt(rmse / n)\n",
    "    print(f\"MAE: {mae:.2f}, RMSE: {rmse:.2f}\")\n",
    "    return mae, rmse\n",
    "\n",
    "# -------------------------------\n",
    "# Alert System\n",
    "# -------------------------------\n",
    "def alert_system(model, img_path, crowd_limit=50):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "    ])\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img_resized = img.resize((IMG_WIDTH, IMG_HEIGHT))\n",
    "    img_t = transform(img_resized).unsqueeze(0).to(DEVICE)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(img_t)\n",
    "    count = max(int(output.sum().item()), 0)\n",
    "    if count > crowd_limit:\n",
    "        print(f\"ðŸš¨ ALERT: Overcrowded! Count: {count} (Limit: {crowd_limit})\")\n",
    "        return True, count\n",
    "    else:\n",
    "        print(f\"âœ… Normal crowd level. Count: {count} (Limit: {crowd_limit})\")\n",
    "        return False, count\n",
    "\n",
    "def process_images_with_alerts(model, img_dir, crowd_limit=50):\n",
    "    image_files = glob.glob(os.path.join(img_dir, \"*.jpg\"))\n",
    "    alert_count = 0\n",
    "    total_images = len(image_files)\n",
    "    if total_images == 0:\n",
    "        print(f\"No images found in {img_dir}\")\n",
    "        return\n",
    "    for img_path in image_files:\n",
    "        filename = os.path.basename(img_path)\n",
    "        is_alert, count = alert_system(model, img_path, crowd_limit)\n",
    "        status = \"ðŸš¨ ALERT\" if is_alert else \"âœ… NORMAL\"\n",
    "        print(f\"{filename:<20} | Count: {count:>4} | {status}\")\n",
    "        if is_alert:\n",
    "            alert_count += 1\n",
    "    print(f\"Summary: {alert_count}/{total_images} images triggered alerts\")\n",
    "    print(f\"Alert percentage: {(alert_count/total_images)*100:.1f}%\")\n",
    "\n",
    "# -------------------------------\n",
    "# Load Model\n",
    "# -------------------------------\n",
    "def load_model(model_path=\"csrnet_partA_best.pth\"):\n",
    "    model = CSRNet().to(DEVICE)\n",
    "    if os.path.exists(model_path):\n",
    "        state_dict = torch.load(model_path, map_location=DEVICE)\n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "        print(f\"Loaded model from {model_path} (strict=False)\")\n",
    "        return model, True\n",
    "    return model, False\n",
    "\n",
    "# -------------------------------\n",
    "# Main\n",
    "# -------------------------------\n",
    "def main():\n",
    "    model, model_loaded = load_model()\n",
    "    if not model_loaded:\n",
    "        print(\"Training new model...\")\n",
    "        model = train_model(num_epochs=5, batch_size=4, learning_rate=1e-5)\n",
    "    evaluate_model(model)\n",
    "    if os.path.exists(TEST_IMG_DIR):\n",
    "        process_images_with_alerts(model, TEST_IMG_DIR, crowd_limit=30)\n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbd329c-8e8c-4a35-a3af-338310fd549c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from scipy.io import loadmat\n",
    "import torch.nn.functional as F\n",
    "from ultralytics import YOLO  # âœ… Added for class filtering (YOLOv8)\n",
    "\n",
    "# -------------------------------\n",
    "# Config\n",
    "# -------------------------------\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "TRAIN_IMG_DIR = r\"D:\\Task For Infosys Internship\\archive\\ShanghaiTech\\part_A\\train_data\\images\"\n",
    "TRAIN_GT_DIR = r\"D:\\Task For Infosys Internship\\archive\\ShanghaiTech\\part_A\\train_data\\ground-truth\"\n",
    "TEST_IMG_DIR  = r\"D:\\Task For Infosys Internship\\archive\\ShanghaiTech\\part_A\\test_data\\images\"\n",
    "TEST_GT_DIR   = r\"D:\\Task For Infosys Internship\\archive\\ShanghaiTech\\part_A\\test_data\\ground-truth\"\n",
    "\n",
    "IMG_HEIGHT = 256\n",
    "IMG_WIDTH = 256\n",
    "DOWNSAMPLE_FACTOR = 8\n",
    "OUTPUT_SIZE = IMG_HEIGHT // DOWNSAMPLE_FACTOR\n",
    "\n",
    "# -------------------------------\n",
    "# Gaussian Density Map Generator\n",
    "# -------------------------------\n",
    "def gaussian_filter_density(gt):\n",
    "    density = np.zeros(gt.shape, dtype=np.float32)\n",
    "    pts = np.array(list(zip(np.nonzero(gt)[1], np.nonzero(gt)[0])))\n",
    "    if len(pts) == 0:\n",
    "        return density\n",
    "    sigma = 15\n",
    "    for i in range(len(pts)):\n",
    "        pt2d = np.zeros(gt.shape, dtype=np.float32)\n",
    "        y, x = pts[i][1], pts[i][0]\n",
    "        if y < gt.shape[0] and x < gt.shape[1]:\n",
    "            pt2d[y, x] = 1.\n",
    "        density += cv2.GaussianBlur(pt2d, (0,0), sigma, borderType=cv2.BORDER_CONSTANT)\n",
    "    return density\n",
    "\n",
    "# -------------------------------\n",
    "# Dataset\n",
    "# -------------------------------\n",
    "class CrowdDataset(Dataset):\n",
    "    def __init__(self, img_dir, gt_dir, transform=None, img_size=(IMG_HEIGHT, IMG_WIDTH)):\n",
    "        self.img_paths = glob.glob(os.path.join(img_dir, \"*.jpg\"))\n",
    "        self.gt_dir = gt_dir\n",
    "        self.transform = transform\n",
    "        self.img_size = img_size\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        filename_mat = os.path.basename(img_path).replace(\".jpg\", \".mat\")\n",
    "        filename_h5  = os.path.basename(img_path).replace(\".jpg\", \".h5\")\n",
    "        mat_path     = os.path.join(self.gt_dir, \"GT_\" + filename_mat)\n",
    "        h5_path      = os.path.join(self.gt_dir, \"GT_\" + filename_h5)\n",
    "        \n",
    "        if os.path.exists(h5_path):\n",
    "            with h5py.File(h5_path, 'r') as hf:\n",
    "                density = np.asarray(hf['density'])\n",
    "                img = img.resize(self.img_size)\n",
    "                if self.transform:\n",
    "                    img = self.transform(img)\n",
    "                density = cv2.resize(density, self.img_size, interpolation=cv2.INTER_LINEAR)\n",
    "                density = torch.from_numpy(density).unsqueeze(0).float()\n",
    "                return img, density\n",
    "        elif os.path.exists(mat_path):\n",
    "            try:\n",
    "                mat = loadmat(mat_path)\n",
    "                points = mat['image_info'][0,0]['location'][0,0]\n",
    "            except NotImplementedError:\n",
    "                with h5py.File(mat_path, 'r') as f:\n",
    "                    points = np.array(f['image_info'][0,0][0,0][0])\n",
    "            if points.size == 0:\n",
    "                coords = np.empty((0, 2), dtype=np.int32)\n",
    "            else:\n",
    "                coords = np.array(points)\n",
    "            img = img.resize(self.img_size)\n",
    "            h, w = img.size[1], img.size[0]\n",
    "            k = np.zeros((h, w))\n",
    "            for i in range(coords.shape[0]):\n",
    "                x = min(int(coords[i][0]), w-1)\n",
    "                y = min(int(coords[i][1]), h-1)\n",
    "                if y < h and x < w:\n",
    "                    k[y, x] = 1\n",
    "            density = gaussian_filter_density(k)\n",
    "            scale_factor = (self.img_size[0] * self.img_size[1]) / (h * w)\n",
    "            density = cv2.resize(density, self.img_size, interpolation=cv2.INTER_LINEAR)\n",
    "            density = density * scale_factor\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            density = torch.from_numpy(density).unsqueeze(0).float()\n",
    "            return img, density\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Ground truth file not found: {mat_path} or {h5_path}\")\n",
    "\n",
    "# -------------------------------\n",
    "# CSRNet Model\n",
    "# -------------------------------\n",
    "class CSRNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CSRNet, self).__init__()\n",
    "        vgg = models.vgg16_bn(weights=models.VGG16_BN_Weights.DEFAULT)\n",
    "        self.frontend = nn.Sequential(*list(vgg.features.children())[:33]) \n",
    "        self.backend = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, 3, padding=2, dilation=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, 3, padding=2, dilation=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, 3, padding=2, dilation=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 256, 3, padding=2, dilation=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 128, 3, padding=2, dilation=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 64, 3, padding=2, dilation=2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.output_layer = nn.Conv2d(64, 1, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        x = self.frontend(x)\n",
    "        x = self.backend(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "# -------------------------------\n",
    "# Class Filtering: Only \"person\"\n",
    "# -------------------------------\n",
    "print(\"Loading YOLOv8 model for person filtering...\")\n",
    "person_detector = YOLO(\"yolov8n.pt\")  # Automatically downloads pretrained model on first run\n",
    "\n",
    "def filter_persons(image_pil):\n",
    "    \"\"\"\n",
    "    Detect and mask only 'person' regions from image.\n",
    "    Returns masked image (same size as input).\n",
    "    \"\"\"\n",
    "    img_cv = np.array(image_pil)[:, :, ::-1]  # Convert PIL -> OpenCV (BGR)\n",
    "    results = person_detector(img_cv, verbose=False)\n",
    "    mask = np.zeros(img_cv.shape[:2], dtype=np.uint8)\n",
    "\n",
    "    for box in results[0].boxes:\n",
    "        cls_id = int(box.cls)\n",
    "        if results[0].names[cls_id] == \"person\":\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "            cv2.rectangle(mask, (x1, y1), (x2, y2), 255, -1)\n",
    "\n",
    "    masked_img = cv2.bitwise_and(img_cv, img_cv, mask=mask)\n",
    "    masked_pil = Image.fromarray(cv2.cvtColor(masked_img, cv2.COLOR_BGR2RGB))\n",
    "    return masked_pil\n",
    "\n",
    "# -------------------------------\n",
    "# Training\n",
    "# -------------------------------\n",
    "def train_model(num_epochs=5, batch_size=4, learning_rate=1e-5, save_every=5):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "    ])\n",
    "    train_dataset = CrowdDataset(TRAIN_IMG_DIR, TRAIN_GT_DIR, transform=transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    model = CSRNet().to(DEVICE)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "    print(f\"Training for {num_epochs} epochs...\")\n",
    "    best_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        batch_count = 0\n",
    "        for imgs, densities in train_loader:\n",
    "            imgs, densities = imgs.to(DEVICE), densities.to(DEVICE)\n",
    "            target_densities_downsampled = F.interpolate(\n",
    "                densities, \n",
    "                size=(OUTPUT_SIZE, OUTPUT_SIZE), \n",
    "                mode='bilinear', \n",
    "                align_corners=False\n",
    "            )\n",
    "            target_densities_downsampled = target_densities_downsampled * (DOWNSAMPLE_FACTOR ** 2)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, target_densities_downsampled) \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            batch_count += 1\n",
    "        avg_loss = epoch_loss / batch_count\n",
    "        scheduler.step()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.6f}, LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(model.state_dict(), \"csrnet_partB_best.pth\")\n",
    "            print(f\"Best model saved with loss: {best_loss:.6f}\")\n",
    "        if (epoch + 1) % save_every == 0:\n",
    "            torch.save(model.state_dict(), f\"csrnet_partB_epoch_{epoch+1}.pth\")\n",
    "    torch.save(model.state_dict(), \"csrnet_partB_final.pth\")\n",
    "    print(\"Final model saved to csrnet_partB_final.pth\")\n",
    "    return model\n",
    "\n",
    "# -------------------------------\n",
    "# Evaluation\n",
    "# -------------------------------\n",
    "def evaluate_model(model):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "    ])\n",
    "    val_dataset = CrowdDataset(TEST_IMG_DIR, TEST_GT_DIR, transform=transform)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "    print(\"Evaluating model...\")\n",
    "    mae, rmse, n = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, densities in val_loader:\n",
    "            imgs, densities = imgs.to(DEVICE), densities.to(DEVICE)\n",
    "            outputs = model(imgs)\n",
    "            predicted_count = max(outputs.sum().item(), 0)\n",
    "            actual_count = densities.sum().item()\n",
    "            mae += abs(predicted_count - actual_count)\n",
    "            rmse += (predicted_count - actual_count) ** 2\n",
    "            n += 1\n",
    "    mae /= n\n",
    "    rmse = np.sqrt(rmse / n)\n",
    "    print(f\"MAE: {mae:.2f}, RMSE: {rmse:.2f}\")\n",
    "    return mae, rmse\n",
    "\n",
    "# -------------------------------\n",
    "# Alert System (with class filtering)\n",
    "# -------------------------------\n",
    "def alert_system(model, img_path, crowd_limit=50):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "    ])\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    filtered_img = filter_persons(img)  # âœ… Only keep \"person\" regions\n",
    "    img_resized = filtered_img.resize((IMG_WIDTH, IMG_HEIGHT))\n",
    "    img_t = transform(img_resized).unsqueeze(0).to(DEVICE)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(img_t)\n",
    "    count = max(int(output.sum().item()), 0)\n",
    "    if count > crowd_limit:\n",
    "        print(f\"ðŸš¨ ALERT: Overcrowded! Count: {count} (Limit: {crowd_limit})\")\n",
    "        return True, count\n",
    "    else:\n",
    "        print(f\"âœ… Normal crowd level. Count: {count} (Limit: {crowd_limit})\")\n",
    "        return False, count\n",
    "\n",
    "def process_images_with_alerts(model, img_dir, crowd_limit=50):\n",
    "    image_files = glob.glob(os.path.join(img_dir, \"*.jpg\"))\n",
    "    alert_count = 0\n",
    "    total_images = len(image_files)\n",
    "    if total_images == 0:\n",
    "        print(f\"No images found in {img_dir}\")\n",
    "        return\n",
    "    for img_path in image_files:\n",
    "        filename = os.path.basename(img_path)\n",
    "        is_alert, count = alert_system(model, img_path, crowd_limit)\n",
    "        status = \"ðŸš¨ ALERT\" if is_alert else \"âœ… NORMAL\"\n",
    "        print(f\"{filename:<20} | Count: {count:>4} | {status}\")\n",
    "        if is_alert:\n",
    "            alert_count += 1\n",
    "    print(f\"Summary: {alert_count}/{total_images} images triggered alerts\")\n",
    "    print(f\"Alert percentage: {(alert_count/total_images)*100:.1f}%\")\n",
    "\n",
    "# -------------------------------\n",
    "# Load Model\n",
    "# -------------------------------\n",
    "def load_model(model_path=\"csrnet_partA_best.pth\"):\n",
    "    model = CSRNet().to(DEVICE)\n",
    "    if os.path.exists(model_path):\n",
    "        state_dict = torch.load(model_path, map_location=DEVICE)\n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "        print(f\"Loaded model from {model_path} (strict=False)\")\n",
    "        return model, True\n",
    "    return model, False\n",
    "\n",
    "# -------------------------------\n",
    "# Main\n",
    "# -------------------------------\n",
    "def main():\n",
    "    model, model_loaded = load_model()\n",
    "    if not model_loaded:\n",
    "        print(\"Training new model...\")\n",
    "        model = train_model(num_epochs=5, batch_size=4, learning_rate=1e-5)\n",
    "    evaluate_model(model)\n",
    "    if os.path.exists(TEST_IMG_DIR):\n",
    "        process_images_with_alerts(model, TEST_IMG_DIR, crowd_limit=30)\n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
